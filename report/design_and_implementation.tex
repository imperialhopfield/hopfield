\chapter{Design and Implementation}
% Justify design decisions, tools used, etc.
% NO CODE!!! (except for skeleton of algorithms maybe)

\section{Technologies used}

%Haskell
We have combined several technologies in order to create a nice workflow for our working environment. First of all, it is worth mentioning that we implemented the core algorithms for the neural network in \textbf{Haskell}. It is a very good candidate for solving mathematical problems, and has a strong type system that helped us detect bugs early on, at compile time.

%Image loading with C
The functions responsible for image preprocessing (loading, rescaling, converting to gray-scale values, mapping pixels to binary patterns) were implemented in the \textbf{C} programming language, using the MagickWand Library. Therefore, we also set up an interface of communication between C and Haskell.

%Git
Since this was a group project in which all of us has to implement various parts of the system, we used Git as a version-control system. We have chosen Git because of it's powerful features and reliability as a distributed version-control system.

%TODO. Talk about Jenkins , continuous integration, Review Board, ....




\section{Technical Challenges}

\subsection{Computation Time}


\hilight{maybe move detailed discussion to where the algorithm is actually described?}
The common theme of our experiments is measuring and comparing basins of attractions over a multitude of parameters, performed using the Storkey-Valabregue method. \hilight{CROSS REF} As we know this involves performing a very expensive computation: sampling 100 patterns for each Hamming radius, a maximum of N steps, and for each of those patterns iterative updates are run with the network until convergence is achieved. Though impossible due to properties of Hopfield networks, we can approximate the worst case complexity of the latter as requiring $2^N$ iterations until convergence, $2^N$ being the number of possible states. Thus we may cynically, for the sake of quantifying it, approximate the worst case complexity of the Storkey-Valabregue method to be $O(N2^N)$. While this is probably not an accurate representation of what occurs in practice, it serves to demonstrate the sheer amount of computation performed.

Naturally, this posed a significant computational challenge which we had to be overcome, as we typically obtain a large number of samples of basin measurements for our experiments, in the order of hundreds of samples for a given experiment. One way in which we overcome this limitation is by exploiting parallelism both at the process level (trivially achieved using \textbf{Haskell}!) and at the machine level, running multiple experiments on several lab machines. We also sought to improve the quality of our code itself, using profiling to identify hotspots. \hilight{next subsection on benchmarks}


\subsection{Big Data}

Another challenge which we have faced is dealing the sheer amount of data generated by our experiments. The data typically needs to be collected, processed, and presented in a consistent manner. In order to address this issue, we sought to automate such tasks. Automation is key, as it saves time, is not prone to (low level) human error, and generally scales well. Achieving this goals required the joint cooperation of three areas:

\begin{itemize}
\item Experiment executables should output data in a sensible and easy to parse format. Ideally, the output data should not be heavily processed so as to allow flexible use of it.

\item Writing scripts which format and aggregate data, typically calculating the mean value.

\item Presentation of data in the form of graphs of tables ought to be consistent. Ideally, the data should be kept separate from the format or design of its final presentation form. The \texttt{pgfplots} package for \LaTeX was very helpful for this.
\end{itemize}



\section{Anticipated Risks}

\subsection{Failure to match a pattern stored}
One of the risks associated with using the Hopfield model is that the converged pattern may not be one of the stored patterns, but rather a spurious pattern, as explained in \hilight{CROSS REF}. This poses a problem for our recognition application, which needs to return as a result the closest matched stored pattern. Another issue with similar repercussions is that a large number of iterations are run, but the pattern has not yet converged.
One solution to this which we have employed \hilight{have we? also cross ref if applicable} is to simply output the stored pattern with the smallest Hamming distance between it and the current state of the input pattern following iteration. This makes our image recognition robust to such unfortunate occurrences.


\subsection{A stored pattern not a fixed point}
It is possible that the given set of training patterns are such that some of the patterns simply cannot be ``stored'' in the network, i.e. it is not a fixed point of the netowkr. The probability of this occurring increases as the relative number of training patterns with respect to the number of neurons increases \hilight{cross ref to capacity}.
The best way to deal with this problem is to perform a check (using \texttt{checkFixed}) \hilight{make sure we do that} and, if detected, report it  to the user and allow them to take action. The typical remedy for this would be to increase the number of neurons in the network, at the expense of increased resource usage in both space and time.


\subsection{Floating point calculation}
As we are dealing with extensive calculations of floating point value, we must be careful when dealing with such quantities. In particular, equality comparisons are likely to yield unexpected results. This has had an impact on some tests as well as functions such as \texttt{validWeights}. We have taken care of this problem by replacing equality comparisons and similar with fuzzy equality comparisons, with some accepted margin of error $\epsilon$.
