% This is the bibliography file
% See	 http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#BibTeX

% Google Scholar has a very handy 'Import into BibTeX' link.
% Just find the paper you want on Google Scholar, then click 'Cite'

@unpublished{lectureslides,
    author    = "Abbas Edalat",
    title     = "Hopfield Networks",
    year     = "2012",
    note     = "Part of Complex Systems course at Imperial College London",
}
,
@article{storkey1999basins,
  title={The basins of attraction of a new {Hopfield} learning rule},
  author={Storkey, A.J. and Valabregue, R.},
  journal={Neural Networks},
  volume={12},
  number={6},
  pages={869--876},
  year={1999},
  publisher={Elsevier}
}
,
@article{net_model_neuroses,
  title={Generalized memory associativity in a network model for the neuroses},
  author={Wedemann, R.S. and Donangelo, R. and de Carvalho, L.A.V.},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={19},
  number={1},
  pages={015116--015116},
  year={2009},
  publisher={American Institute of Physics}
}
,
@misc{website:attachment_theory_wiki,
  author = "Wikipedia",
  title = "Attachment Theory",    
  url = "http://en.wikipedia.org/wiki/Attachment_theory"
}
,
@misc{website:attachment_patterns_wiki,
  author = "Wikipedia",
  title = "Attachment Patterns",    
  url = "http://en.wikipedia.org/wiki/Attachment_theory#Attachment_patterns"
}
,
@article{nets_parkinsons,
  title={Diagnosing parkinson by using artificial neural networks and support vector machines},
  author={Gil, D. and Manuel, D.J.},
  journal={Global Journal of Computer Science and Technology},
  volume={9},
  number={4},
  year={2009}
}
,
@article{hopfield_laferriere,
  title="Hopfield Network",
  author="Alice Julien Lafferiere"
}
,
@article{speech_nets,
  title={Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition},
  author={Dahl, G.E. and Yu, D. and Deng, L. and Acero, A.},
  journal={Audio, Speech, and Language Processing, IEEE Transactions on},
  volume={20},
  number={1},
  pages={30--42},
  year={2012},
  publisher={IEEE}
}
,
@article{hopfield_wiki,
  title ="Hopfield Network",
  author ="Wikipedia",
  url = "http://en.wikipedia.org/wiki/Hopfield_network"
}
,
@article{hebb_wiki,
  title ="Hebbian Theory",
  author ="Wikipedia",
  url = "http://en.wikipedia.org/wiki/Hebbian_theory"
}
,
@article{storkey1997increasing,
  title={Increasing the capacity of a Hopfield network without sacrificing functionality},
  author={Storkey, A.},
  journal={Artificial Neural Networksâ€”ICANN'97},
  pages={451--456},
  year={1997},
  publisher={Springer}
}
,
@unpublished{federico,
  title={Modelling Attachment Types with Hopfield Neural Network},
  author={Federico Mancinelli},
  year={2012},
  note={Master Thesys for Computing, Imperial College}
}
,
@article{louradour2011classification,
  title={Classification of sets using restricted Boltzmann machines},
  author={Louradour, J. and Larochelle, H.},
  journal={arXiv preprint arXiv:1103.4896},
  year={2011}
}
,
@article{teh2001rate,
  title={Rate-coded restricted Boltzmann machines for face recognition},
  author={Teh, Y.W. and Hinton, G.E.},
  journal={Advances in neural information processing systems},
  pages={908--914},
  year={2001},
  publisher={MIT; 1998}
}
,
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, V. and Hinton, G.E.},
  booktitle={Proc. 27th International Conference on Machine Learning},
  pages={807--814},
  year={2010},
  organization={Omnipress Madison, WI}
}
,
@inproceedings{larochelle2008classification,
  title={Classification using discriminative restricted Boltzmann machines},
  author={Larochelle, H. and Bengio, Y.},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={536--543},
  year={2008},
  organization={ACM}
}
,
@article{hinton2010practical,
  title={A practical guide to training restricted Boltzmann machines},
  author={Hinton, G.},
  journal={Momentum},
  volume={9},
  pages={1},
  year={2010}
}


