\chapter{Introduction}
% Tony: "1, 2, 3 pages"
% What is the problem?
% Why is it interesting?
% How did you solve it?
% How far did you get?

% Includes motivation, objectives, and contributions (what you achieved)
\section{Motivation}
The human brain is one of the most complex objects, that researchers have been trying to analyse since antiquity. Understanding how it works, by performing experiments and modelling it would help us cure various brain and mental disorders. 

Several medical discoveries from the 20th century have enlightened our knowledge of the human brain, and therefore different mathematical models of artificial neural networks have been created, inspired from biology and medicine. 

Our motivation is to use some of these neural networks to explore the mathematics of a developing theory called Self-Attachment. This theory is aiming to help curing various mental problems that people are currently facing.Recent research has shown that a mathematical way of analysing these disorders is starting to become feasible. \cite{net_model_neuroses}

The subsequent chapters will introduce the Attachment Theory and the mathematical model. We have mainly focused on the technical side, and described  psychological analogies that explain our technical results. 

\section{Objectives}

Our main objective is to confirm the results of previous work done by Federico Macinelli, who has been analysing the attachment theory using neural networks. He has performed several experiments regarding clusters of attractors, basin sizes, and gaussian-distributed patterns. Furtheremore, we have been aiming at extending his results by exploring the following concepts:
\begin{itemize}
\item Using the network for performing image recognition
\item Restricted Bolzman machines
\item Super-attractors
\end{itemize}


In addition to that, we were aiming to improve some of the methods that were used in his experiments. This includes the technique for sampling gaussian-distributed patterns or for calculating basin sizes. Our final aims consisted of explaining some of the subsequeny inconsistencies that have been found in his results. 

\section{Contributions}

Since our project was related to exploring attractor-based neural networks, we have obtained interesting results about their attractors. These attractors are analogous to learned memories or experiences that an individual has learned. 


Our main contributions are outlined below:
\begin{itemize}
\item Confirming Federico's results by reimplementing them in a completely different evironment (Haskell)
\item Proving that the Hopfield network is capable of performing image recognition, by learning image patterns and then recalling the closest image, when queried for an input. Furthermore, we have proven it's associative memory properties, by recalling some of the learned images. 
\item We found out that training patterns are not guaranteed to become fixed points in the Hopfield network. This is an aspect that was not mentioned in simmilar research papers, and we first thought the the patterns are always fixed points.
\item Extending some of the experiments to the Bolzman Machine, which provides a nice way of overcoming some limitations of the Hopfield Network. It can prevent convergence to spurious patterns by using a stochasting update rule. 
\item A thorough analysis of Super-Attractors, which represent patterns that have been used multiple times in the training process. They generally have greater basin sizes, and therefore patterns will have a greater chance of converging to them compared to normal attractors. 
\end{itemize}
