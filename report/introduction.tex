\chapter{Introduction}
% Tony: "1, 2, 3 pages"
% What is the problem?
% Why is it interesting?
% How did you solve it?
% How far did you get?

% Includes motivation, objectives, and contributions (what you achieved)
\section{Motivation}

%Understanding the human brain
The human brain is considered to be one of the most complex objects which have puzzled scientists and philosophers alike throughout the centuries. To gain an understanding of how this mysterious black box works, through experimentation and modelling, would be of great aid to the furthering of science and humanity alike. A part of this includes developing cures for a multitude of mental and brain-related disorders.

%Artificial neural networks
Several medical discoveries from the 20th century have enlightened our knowledge of the human brain. One consequence of this was the development of different mathematical models of artificial neural networks, inspired by the fields of biology and medicine.

% modelling the Attachment Theory. Watch out between the distinction between Self-Attachment and Attachment Theory
Our motivation is to use some of these neural networks to explore the mathematics of a developing theory called Self-Attachment. Part of Attachment theory, this strategy aims to help cure various mental problems that people currently facing. Recent research has shown that a mathematical methodology of analysing these disorders is starting to become feasible. \cite{net_model_neuroses}

The subsequent chapters will introduce the Attachment Theory and the mathematical model. We have mainly focused on the technical side, and have described psychological analogies that place our technical results in context of the theory.

\section{Objectives}

%Main Objectives
Our main objective is to confirm the results of previous work done by Federico Macinelli, who has been analysing the attachment theory using neural networks. He has performed several experiments regarding clusters of attractors, basin sizes, and Gaussian-distributed patterns. Furthermore, we have been aiming at extending his results by exploring the following concepts:
\begin{itemize}
\item Using the network for performing image recognition
\item Restricted Boltzmann machines
\item Super-attractors
\end{itemize}

%Improvement of Federico's results
In addition to that, we were aiming to improve some of the methods that were used in his experiments. This includes the technique for sampling Gaussian-distributed patterns or for calculating basin sizes. Our final aims consisted of explaining some of the subsequent inconsistencies that have been found in his results.

\section{Contributions}

Since our project was related to exploring attractor-based neural networks, we have obtained interesting results about their attractors. These attractors are analogous to learned memories or experiences that an individual has learned.

%This list is by no means exhaustive. Feel free to add any other aspects. This is what I(Raz) could recall a few days ago.
Our main contributions are outlined below:
\begin{itemize}
\item Confirming Federico's results by reimplementing them in a completely different environment (Haskell)
\item Proving that the Hopfield network is capable of performing image recognition, by learning image patterns and then recalling the closest image, when queried for an input. Furthermore, we have proven it's associative memory properties, by recalling some of the learned images.
\item We found out that training patterns are not guaranteed to become fixed points in the Hopfield network. This is an aspect that was not mentioned in similar research papers, and we first thought the the patterns are always fixed points.
\item Extending some of the experiments to the Boltzmann Machine, which provides a nice way of overcoming some limitations of the Hopfield Network. It can prevent convergence to spurious patterns by using a stochastic update rule.
\item A thorough analysis of Super-Attractors, which represent patterns that have been used multiple times in the training process. They generally have greater basin sizes, and therefore patterns will have a greater chance of converging to them compared to normal attractors.
\end{itemize}
